model:
  training_config:
    lr: 3e-4
    weight_decay: 1e-4
    warmup_steps: 16_000
  # Don't know why but linking these arguments is not working
  tokenizer_config:
    lang: en
    dim: 300
    vocab_size: 10_000

data:
  tokenizer_config:
    lang: en
    dim: 300
    vocab_size: 10_000

trainer:
  max_epochs: 100
  fast_dev_run: false
  log_every_n_steps: 100
  logger:
    - class_path: WandbLogger
      init_args:
        project: gpet
        dir: wandb/
  callbacks:
    - class_path: RichModelSummary
      init_args:
        max_depth: 3
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: pytorch_lightning.callbacks.early_stopping.EarlyStopping
      init_args:
        monitor: val/loss
        mode: min
        patience: 3
        verbose: true
        min_delta: 0.01
    - class_path: ModelCheckpoint
      init_args:
        save_top_k: 3
        save_last: true
        monitor: val/loss